{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# ================================================\n# KAGGLE LLM CLASSIFICATION COMPETITION\n# FIRST SUBMISSION - SIMPLE BASELINE\n# ================================================\n# ================================================\n# KAGGLE LLM CLASSIFICATION COMPETITION\n# FIRST SUBMISSION - CORRECTED VERSION\n# ================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"LLM CLASSIFICATION COMPETITION - FIRST SUBMISSION\")\nprint(\"=\"*50)\n\n# Load the competition data\nprint(\"\\nüìÇ Loading data...\")\ntrain_path = '/kaggle/input/llm-classification-finetuning/train.csv'\ntest_path = '/kaggle/input/llm-classification-finetuning/test.csv'\n\nif not os.path.exists(train_path):\n    print(\"‚ùå ERROR: train.csv not found!\")\nelse:\n    # Load the data\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    \n    print(f\"‚úÖ Data loaded successfully!\")\n    print(f\"   Training samples: {len(train_df):,}\")\n    print(f\"   Test samples: {len(test_df):,}\")\n    \n    # Display basic info\n    print(\"\\nüìä Training data columns:\")\n    print(f\"   {list(train_df.columns)}\")\n    \n    print(\"\\nüìä Test data columns:\")\n    print(f\"   {list(test_df.columns)}\")\n    \n    print(\"\\nüìã Sample of training data:\")\n    print(train_df.head())\n    \n    print(\"\\nüìã Sample of test data:\")\n    print(test_df.head())\n    \n    # ================================================\n    # UNDERSTAND THE TARGET\n    # ================================================\n    print(\"\\nüéØ Understanding target columns...\")\n    print(\"We have THREE target columns:\")\n    print(\"1. winner_model_a - Probability model A wins\")\n    print(\"2. winner_model_b - Probability model B wins\") \n    print(\"3. winner_tie - Probability of tie\")\n    \n    # Check target distribution in training data\n    print(\"\\nüìà Training target summary:\")\n    print(f\"   winner_model_a mean: {train_df['winner_model_a'].mean():.3f}\")\n    print(f\"   winner_model_b mean: {train_df['winner_model_b'].mean():.3f}\")\n    print(f\"   winner_tie mean: {train_df['winner_tie'].mean():.3f}\")\n    \n    # Show samples of actual targets\n    print(\"\\nüìä Sample targets from training data:\")\n    sample_targets = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].head()\n    print(sample_targets)\n    \n    # ================================================\n    # SIMPLE BASELINE MODEL\n    # ================================================\n    print(\"\\n\" + \"=\"*50)\n    print(\"CREATING SIMPLE BASELINE MODEL\")\n    print(\"=\"*50)\n    \n    # Strategy 1: Use average probabilities from training data\n    print(\"\\nü§ñ Strategy 1: Using training averages...\")\n    \n    avg_model_a = train_df['winner_model_a'].mean()\n    avg_model_b = train_df['winner_model_b'].mean() \n    avg_tie = train_df['winner_tie'].mean()\n    \n    print(f\"   Average probabilities from training:\")\n    print(f\"   Model A wins: {avg_model_a:.3f}\")\n    print(f\"   Model B wins: {avg_model_b:.3f}\")\n    print(f\"   Tie: {avg_tie:.3f}\")\n    \n    # Create submission with average probabilities\n    submission_avg = pd.DataFrame({\n        'id': test_df['id'],\n        'winner_model_a': [avg_model_a] * len(test_df),\n        'winner_model_b': [avg_model_b] * len(test_df),\n        'winner_tie': [avg_tie] * len(test_df)\n    })\n    \n    # Strategy 2: Length-based heuristic\n    print(\"\\nüìè Strategy 2: Length-based heuristic...\")\n    \n    def get_length_based_probabilities(response_a, response_b):\n        \"\"\"Simple heuristic based on response length\"\"\"\n        len_a = len(str(response_a))\n        len_b = len(str(response_b))\n        \n        # Calculate length ratio\n        total_len = len_a + len_b\n        if total_len == 0:\n            return 0.33, 0.33, 0.34\n        \n        prob_a = len_a / total_len * 0.8 + 0.1  # Scale to reasonable range\n        prob_b = len_b / total_len * 0.8 + 0.1\n        prob_tie = 1.0 - prob_a - prob_b\n        \n        # Ensure valid probabilities\n        prob_tie = max(0.1, min(0.8, prob_tie))\n        scale = 1.0 / (prob_a + prob_b + prob_tie)\n        prob_a *= scale\n        prob_b *= scale\n        prob_tie *= scale\n        \n        return prob_a, prob_b, prob_tie\n    \n    # Apply to test data\n    length_predictions = test_df.apply(\n        lambda row: get_length_based_probabilities(row['response_a'], row['response_b']),\n        axis=1\n    )\n    \n    submission_length = pd.DataFrame({\n        'id': test_df['id'],\n        'winner_model_a': [p[0] for p in length_predictions],\n        'winner_model_b': [p[1] for p in length_predictions],\n        'winner_tie': [p[2] for p in length_predictions]\n    })\n    \n    # ================================================\n    # CHOOSE FINAL SUBMISSION\n    # ================================================\n    print(\"\\n\" + \"=\"*50)\n    print(\"FINAL SUBMISSION CREATION\")\n    print(\"=\"*50)\n    \n    # Let's use length-based (more interesting)\n    final_submission = submission_length.copy()\n    \n    print(\"\\nüìã Final submission preview:\")\n    print(final_submission.head())\n    \n    # Verify probabilities sum to 1\n    final_submission['sum_check'] = (\n        final_submission['winner_model_a'] + \n        final_submission['winner_model_b'] + \n        final_submission['winner_tie']\n    )\n    \n    print(f\"\\n‚úÖ Submission verification:\")\n    print(f\"   Shape: {final_submission.shape}\")\n    print(f\"   All probabilities sum to 1? {np.allclose(final_submission['sum_check'], 1.0, rtol=1e-10)}\")\n    print(f\"   Min sum: {final_submission['sum_check'].min():.10f}\")\n    print(f\"   Max sum: {final_submission['sum_check'].max():.10f}\")\n    \n    # Show distribution of predictions\n    print(f\"\\nüìä Prediction summary:\")\n    print(f\"   Model A win probability: mean={final_submission['winner_model_a'].mean():.3f}, std={final_submission['winner_model_a'].std():.3f}\")\n    print(f\"   Model B win probability: mean={final_submission['winner_model_b'].mean():.3f}, std={final_submission['winner_model_b'].std():.3f}\")\n    print(f\"   Tie probability: mean={final_submission['winner_tie'].mean():.3f}, std={final_submission['winner_tie'].std():.3f}\")\n    \n    # ================================================\n    # SAVE SUBMISSION FILE\n    # ================================================\n    # Remove the check column\n    final_submission = final_submission.drop('sum_check', axis=1)\n    \n    # Save to CSV\n    output_path = '/kaggle/working/submission.csv'\n    final_submission.to_csv(output_path, index=False)\n    \n    print(f\"\\nüíæ Submission saved to: {output_path}\")\n    print(f\"   File size: {os.path.getsize(output_path):,} bytes\")\n    \n    # Show first few lines of file\n    print(\"\\nüìÑ First 3 lines of submission.csv:\")\n    with open(output_path, 'r') as f:\n        for i, line in enumerate(f):\n            if i < 3:\n                print(f\"   {line.strip()}\")\n            else:\n                break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-11T07:50:41.449202Z","iopub.execute_input":"2026-02-11T07:50:41.449663Z","iopub.status.idle":"2026-02-11T07:50:43.672776Z","shell.execute_reply.started":"2026-02-11T07:50:41.449630Z","shell.execute_reply":"2026-02-11T07:50:43.671593Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nLLM CLASSIFICATION COMPETITION - FIRST SUBMISSION\n==================================================\n\nüìÇ Loading data...\n‚úÖ Data loaded successfully!\n   Training samples: 57,477\n   Test samples: 3\n\nüìä Training data columns:\n   ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']\n\nüìä Test data columns:\n   ['id', 'prompt', 'response_a', 'response_b']\n\nüìã Sample of training data:\n       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n2  [\"explain function calling. how would you call...   \n3  [\"How can I create a test set for a very rare ...   \n4  [\"What is the best way to travel from Tel-Aviv...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n2  [\"Function calling is the process of invoking ...   \n3  [\"Creating a test set for a very rare category...   \n4  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n2  [\"Function calling is the process of invoking ...               0   \n3  [\"When building a classifier for a very rare c...               1   \n4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  \n\nüìã Sample of test data:\n        id                                             prompt  \\\n0   136060  [\"I have three oranges today, I ate an orange ...   \n1   211333  [\"You are a mediator in a heated political deb...   \n2  1233961  [\"How to initialize the classification head wh...   \n\n                                          response_a  \\\n0                    [\"You have two oranges today.\"]   \n1  [\"Thank you for sharing the details of the sit...   \n2  [\"When you want to initialize the classificati...   \n\n                                          response_b  \n0  [\"You still have three oranges. Eating an oran...  \n1  [\"Mr Reddy and Ms Blue both have valid points ...  \n2  [\"To initialize the classification head when p...  \n\nüéØ Understanding target columns...\nWe have THREE target columns:\n1. winner_model_a - Probability model A wins\n2. winner_model_b - Probability model B wins\n3. winner_tie - Probability of tie\n\nüìà Training target summary:\n   winner_model_a mean: 0.349\n   winner_model_b mean: 0.342\n   winner_tie mean: 0.309\n\nüìä Sample targets from training data:\n   winner_model_a  winner_model_b  winner_tie\n0               1               0           0\n1               0               1           0\n2               0               0           1\n3               1               0           0\n4               0               1           0\n\n==================================================\nCREATING SIMPLE BASELINE MODEL\n==================================================\n\nü§ñ Strategy 1: Using training averages...\n   Average probabilities from training:\n   Model A wins: 0.349\n   Model B wins: 0.342\n   Tie: 0.309\n\nüìè Strategy 2: Length-based heuristic...\n\n==================================================\nFINAL SUBMISSION CREATION\n==================================================\n\nüìã Final submission preview:\n        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.246395        0.662696    0.090909\n1   211333        0.643667        0.265424    0.090909\n2  1233961        0.467202        0.441889    0.090909\n\n‚úÖ Submission verification:\n   Shape: (3, 5)\n   All probabilities sum to 1? True\n   Min sum: 1.0000000000\n   Max sum: 1.0000000000\n\nüìä Prediction summary:\n   Model A win probability: mean=0.452, std=0.199\n   Model B win probability: mean=0.457, std=0.199\n   Tie probability: mean=0.091, std=0.000\n\nüíæ Submission saved to: /kaggle/working/submission.csv\n   File size: 243 bytes\n\nüìÑ First 3 lines of submission.csv:\n   id,winner_model_a,winner_model_b,winner_tie\n   136060,0.24639498432601886,0.6626959247648904,0.09090909090909091\n   211333,0.6436667140892494,0.2654241950016598,0.09090909090909091\n","output_type":"stream"}],"execution_count":5}]}